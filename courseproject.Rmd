---
title: "Practical Machine Learning Course Project"
author: "Alexis Carlo J. Ramos"
date: "August 12, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary
The data used in this activity are taken from devices that recorded the physical activities of six participants. Details of the data can be found at <http://groupware.les.inf.puc-rio.br/har>.


## Load and Explore the Data
```{r load, results='hide',message=FALSE}
#load data
data <- read.csv("C:/Users/10012194/Desktop/Coursera/Coursera - PML/pml-training.csv")
head(data)
#remove variables with more than 50% NA's and empty cells
data <- read.csv("C:/Users/10012194/Desktop/Coursera/Coursera - PML/pml-training.csv",
                 na.strings = c("","NA"))
cols <- c();for(col in 1:ncol(data)){if(sum(is.na(data[,col]))>=nrow(data)/2){cols<-c(cols,col)}}
data <- data[,-cols]
#remove irrelevant variables
data <- data[,!colnames(data)%in%c("X","cvtd_timestamp","user_name")]
#split data into training and test sets
library(caret)
set.seed(705)
ind <- createDataPartition(data$classe, p=0.8, list=F)
training <- data[ind,]
testing <- data[-ind,]
```
The data contains 19,622 observations and 160 variables.  
This data will be split into 80-20 portions for the training and testing set respectively.  
Some of the variables contain a lot of empty cells and NA values. These must be removed as these cannot be used as predictors. Also, some variables have little variability, are generated as auto-increments, and are irrelevant. These will be removed as well.


##Training Various Models
```{r train, results=FALSE, eval=FALSE, cache=TRUE}
mdl1 <- train(classe~., data=training, method="rpart") #low accuracy
mdl2 <- train(classe~., data=training, method="gbm")
mdl3 <- train(classe~., data=training, method="lda")
library(e1071)
mdl4 <- svm(classe~., data=training)

pred1 <- predict(mdl1, newdata = testing)
pred2 <- predict(mdl2, newdata = testing)
pred3 <- predict(mdl3, newdata = testing)
pred4 <- predict(mdl6, newdata = testing)

confusionMatrix(pred1, testing$classe) #0.4968
confusionMatrix(pred2, testing$classe) #0.9887
confusionMatrix(pred3, testing$classe) #0.7076
confusionMatrix(pred4, testing$classe) #0.9528
```

Four models were considered for this problem. The four models are Decision Tree, Boosted Trees, Linear Discriminant Analysis, and Support Vector Machine. Boosted Trees scored the highest accuracy, therefore, this will be used to predict the new dataset.

## Testing the Model
```{r final, eval=FALSE}
testdata <- read.csv("C:/Users/10012194/Desktop/Coursera/Coursera - PML/pml-testing.csv")
pred <- predict(mdl2, newdata=testdata)
table(pred)
```

